# Mastodon Spark Pipeline

Projet data streaming : **Mastodon â†’ Kafka â†’ Spark â†’ PostgreSQL â†’ Visualisations**.

## Part 1 â€” Ingestion temps rÃ©el (Mastodon â†’ Kafka)

### PrÃ©requis
- Docker Desktop (Mac/ARM OK)
- Token Mastodon (crÃ©ez une app sur https://mastodon.social)
- Python 3.10+ (pour le producer local)
- Kafka topics: `mastodon_stream` & `mastodon_errors`

### DÃ©marrage
```bash
# 1) Kafka & Zookeeper
cd docker
docker compose up -d

# 2) Topics
docker compose exec kafka bash -lc "kafka-topics --bootstrap-server localhost:9092 --create --if-not-exists --topic mastodon_stream --replication-factor 1 --partitions 1"
docker compose exec kafka bash -lc "kafka-topics --bootstrap-server localhost:9092 --create --if-not-exists --topic mastodon_errors --replication-factor 1 --partitions 1"

# 3) Config env
cd ..
cp .env.example .env   # puis mettre MASTODON_ACCESS_TOKEN

# 4) Producer
cd producer
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python mastodon_producer.py

# Mastodon â†’ Kafka â†’ Spark â†’ Postgres (Partie 2 ValidÃ©e)

Pipeline temps rÃ©el :
- **Producer** Mastodon â†’ **Kafka** (`mastodon_stream`)
- **Spark Structured Streaming** lit Kafka, parse, agrÃ¨ge
- **Postgres** reÃ§oit :
  - `masto.toots_raw` (raw events)
  - `masto.toot_metrics_windowed` (fenÃªtre 1 min par langue)
  - `masto.user_avg_length_windowed` (fenÃªtre 1 min par user)

## PrÃ©requis
- Docker + Docker Compose
- AccÃ¨s Ã  un **token Mastodon** (scopes lecture)

## Arborescence
Batch Processing (Partie 3)
Lancer le job
cd docker
docker compose exec spark bash -lc '
mkdir -p /tmp/.ivy2 && chmod -R 777 /tmp/.ivy2
export PG_URL="jdbc:postgresql://postgres:5432/airflow"
export PG_USER="airflow"
export PG_PASS="airflow"
export ACTIVITY_MIN_TOOTS=5
/opt/spark/bin/spark-submit \
  --master spark://spark:7077 \
  --name MastodonBatchAnalysis \
  --packages org.postgresql:postgresql:42.7.4 \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  /app/batch_job.py
'


## Partie 4 â€” Sentiment Analysis avec Spark MLlib

### Objectif

Cette partie consiste Ã  construire et Ã©valuer un **modÃ¨le dâ€™analyse de sentiments** avec **Spark MLlib**, puis Ã  lâ€™appliquer sur les donnÃ©es Mastodon prÃ©cÃ©demment collectÃ©es.
Les rÃ©sultats sont enregistrÃ©s dans PostgreSQL pour analyse et visualisation.

---

### Ã‰tapes rÃ©alisÃ©es

#### 1. **PrÃ©traitement du texte**

* Nettoyage du texte (`content_txt`)
* Tokenisation des mots avec `RegexTokenizer`
* Suppression des stopwords (`StopWordsRemover`)
* Vectorisation TF-IDF (`HashingTF` + `IDF`)

#### 2. **EntraÃ®nement du modÃ¨le**

* ModÃ¨le : `LogisticRegression`
* Dataset utilisÃ© : `sentiment140` (Kaggle)
* Split 80% / 20% pour lâ€™entraÃ®nement et le test
* Sauvegarde du pipeline entraÃ®nÃ© dans :

  ```
  /data/models/sentiment_pipeline_model
  ```

#### 3. **Ã‰valuation**

Le modÃ¨le a Ã©tÃ© Ã©valuÃ© Ã  lâ€™aide des mÃ©triques suivantes :

| Metric       | Score  |
| :----------- | :----- |
| **AUC**      | 0.8169 |
| **Accuracy** | 0.7590 |
| **F1**       | 0.7590 |

Ces rÃ©sultats sont visibles dans les logs dâ€™exÃ©cution du script `sentiment_train.py`.

#### 4. **Batch Sentiment Analysis**

* Lecture de la table PostgreSQL `masto.toots_raw`
* Application du modÃ¨le sur chaque toot
* Extraction de la probabilitÃ© positive (`prob_pos`)
* Attribution du label :

  * `positive` si `prob_pos >= 0.55`
  * `negative` si `prob_pos <= 0.45`
  * `neutral` sinon
* Ã‰criture dans la table :

  ```
  masto.toots_sentiment
  ```

---

### Structure des tables

#### **Table : masto.toots_sentiment**

| Colonne    | Type      | Description                       |
| :--------- | :-------- | :-------------------------------- |
| id         | bigint    | Identifiant du toot               |
| created_at | timestamp | Date de publication               |
| lang       | text      | Langue                            |
| text       | text      | Contenu textuel                   |
| label_bin  | int       | 0 = neg / 1 = pos                 |
| prob_pos   | float     | ProbabilitÃ© positive              |
| label_str  | text      | `positive`, `neutral`, `negative` |

#### **Vue : masto.v_sentiment_daily**

Vue dâ€™agrÃ©gation journaliÃ¨re des sentiments :

```sql
CREATE OR REPLACE VIEW masto.v_sentiment_daily AS
SELECT date_trunc('day', created_at)::date AS day,
       COUNT(*) AS n_toosts,
       SUM((label_str='positive')::int) AS n_pos,
       SUM((label_str='negative')::int) AS n_neg,
       SUM((label_str='neutral')::int)  AS n_neu,
       AVG(prob_pos) AS avg_prob_pos
FROM masto.toots_sentiment
GROUP BY 1;
```

---

###  ExÃ©cution des scripts

#### 1ï¸ EntraÃ®nement du modÃ¨le

```bash
docker compose exec spark bash -lc '
mkdir -p /tmp/.ivy2 && chmod -R 777 /tmp/.ivy2
export DATA_PATH="/data/training.1600000.processed.noemoticon.csv"
export MODEL_DIR="/data/models/sentiment_pipeline_model"
/opt/spark/bin/spark-submit \
  --master spark://spark:7077 \
  --name MastodonSentimentTrain \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  /app/sentiment_train.py
'
```

 RÃ©sultat attendu :

``` ModÃ¨le enregistrÃ© dans /data/models/sentiment_pipeline_model
AUC: 0.8169
Accuracy: 0.7590
F1: 0.7590
```

#### Batch de prÃ©diction

```bash
docker compose exec spark bash -lc '
mkdir -p /tmp/.ivy2 && chmod -R 777 /tmp/.ivy2
export MODEL_DIR="/data/models/sentiment_pipeline_model"
export PG_URL="jdbc:postgresql://postgres:5432/airflow"
export PG_USER="airflow"
export PG_PASS="airflow"
/opt/spark/bin/spark-submit \
  --master spark://spark:7077 \
  --name MastodonSentimentBatchPredict \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.postgresql:postgresql:42.7.4 \
  /app/sentiment_batch_predict.py
'
```

RÃ©sultat attendu :

```
Batch de prÃ©diction terminÃ© et Ã©crit dans masto.toots_sentiment
```

---

### VÃ©rification dans PostgreSQL

#### Nombre total de toots analysÃ©s :

```bash
docker compose exec postgres bash -lc \
'psql -U airflow -d airflow -c "SELECT COUNT(*) FROM masto.toots_sentiment;"'
```

#### Exemple de rÃ©sultats :

```bash
docker compose exec postgres bash -lc \
'psql -U airflow -d airflow -c "
  SELECT id, created_at, lang,
         LEFT(text, 80) AS text,
         label_bin,
         ROUND(prob_pos::numeric, 3) AS prob_pos,
         label_str
  FROM masto.toots_sentiment
  ORDER BY created_at DESC
  LIMIT 10;"'
```

#### Vue dâ€™agrÃ©gation journaliÃ¨re :

```bash
docker compose exec postgres bash -lc \
'psql -U airflow -d airflow -c "SELECT * FROM masto.v_sentiment_daily ORDER BY day DESC LIMIT 7;"'
```

---

###  Variables dâ€™environnement

| Variable       | Valeur par dÃ©faut                         | Description             |
| :------------- | :---------------------------------------- | :---------------------- |
| `MODEL_DIR`    | `/data/models/sentiment_pipeline_model`   | Chemin du modÃ¨le MLlib  |
| `PG_URL`       | `jdbc:postgresql://postgres:5432/airflow` | Connexion PostgreSQL    |
| `PG_USER`      | `airflow`                                 | Utilisateur PostgreSQL  |
| `PG_PASS`      | `airflow`                                 | Mot de passe PostgreSQL |
| `NEUTRAL_LOW`  | `0.45`                                    | Seuil infÃ©rieur neutre  |
| `NEUTRAL_HIGH` | `0.55`                                    | Seuil supÃ©rieur neutre  |

---

Exactement ðŸ‘Œ tu as tout compris !
ðŸ‘‰ Pas besoin de modifier le CSV pour ton rendu : le graphique vide est **normal** (il reflÃ¨te fidÃ¨lement le fait que tu nâ€™as quâ€™une journÃ©e de donnÃ©es).
Câ€™est mÃªme mieux de le laisser ainsi, car cela montre que **ton pipeline fonctionne jusquâ€™au bout** (extraction â†’ stockage â†’ vue SQL â†’ export CSV â†’ visualisation).
Il suffit juste de **lâ€™expliquer dans ton README**, et câ€™est parfait âœ…

---

### Voici la section prÃªte Ã  coller dans ton `README.md` :

---

## ðŸ§© Partie 5 â€“ Visualisation des RÃ©sultats

Cette derniÃ¨re Ã©tape consiste Ã  **analyser et visualiser les donnÃ©es** collectÃ©es et traitÃ©es par le pipeline Mastodon.
Les fichiers CSV extraits depuis PostgreSQL ont Ã©tÃ© utilisÃ©s pour gÃ©nÃ©rer plusieurs graphiques avec **Matplotlib** et **Pandas**.

### ðŸ“ Emplacement des visuels

Tous les graphiques sont enregistrÃ©s automatiquement dans le dossier :

```
./reports/
```

### ðŸ“Š Graphiques gÃ©nÃ©rÃ©s

| Nom du fichier                         | Description                                                                                                                             |
| -------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **part5_sentiment_distribution.png**   | RÃ©partition globale des sentiments (positif, neutre, nÃ©gatif) sur lâ€™ensemble des toots collectÃ©s.                                       |
| **part5_sentiment_by_day_stacked.png** | Ã‰volution journaliÃ¨re du sentiment avec un graphique empilÃ© permettant de visualiser la proportion de chaque sentiment au fil du temps. |
| **part5_toots_per_day.png**            | Nombre total de toots par jour. *(Dans cet exemple, une seule journÃ©e de donnÃ©es Ã©tait disponible, dâ€™oÃ¹ le graphique quasi vide.)*      |
| **part5_top_hashtags.png**             | Classement des hashtags les plus frÃ©quemment utilisÃ©s dans les toots analysÃ©s.                                                          |

### ðŸ§  InterprÃ©tation

* La majoritÃ© des toots analysÃ©s sont de **sentiment positif**, suivis par des toots nÃ©gatifs et trÃ¨s peu de neutres.
* Les hashtags les plus utilisÃ©s sont liÃ©s Ã  des thÃ©matiques sociales, culturelles ou sportives (ex. *#cheerlights*, *#nowplaying*, *#football*).
* Les volumes journaliers Ã©tant faibles dans cet Ã©chantillon, les tendances temporelles sont limitÃ©es mais dÃ©montrent que le pipeline capture et traite correctement les donnÃ©es.

## ðŸš€ Partie 6 â€“ Conclusion & Perspectives

Ce projet **Mastodon Spark Pipeline** illustre la mise en place complÃ¨te dâ€™une architecture de traitement de donnÃ©es modernes en suivant une approche **ETL (Extract â€“ Transform â€“ Load)** distribuÃ©e.

Lâ€™objectif Ã©tait de :

* collecter en continu des **toots** issus de Mastodon (via Kafka et un producteur Python),
* les stocker dans **PostgreSQL**,
* les analyser avec **Apache Spark** pour rÃ©aliser une **analyse de sentiment**,
* et enfin, visualiser les rÃ©sultats Ã  travers plusieurs tableaux de bord.

### ðŸ§­ Bilan du projet

* âœ… Le pipeline fonctionne de bout en bout : ingestion, stockage, traitement et visualisation.
* âœ… Les modÃ¨les de Machine Learning (sentiment analysis) ont Ã©tÃ© entraÃ®nÃ©s et appliquÃ©s avec succÃ¨s.
* âœ… Les vues SQL et exports CSV permettent de consolider facilement les indicateurs.
* âœ… Les visualisations finales illustrent la rÃ©partition des sentiments, lâ€™Ã©volution temporelle et les hashtags dominants.

### ðŸŒ± Perspectives dâ€™amÃ©lioration

* IntÃ©grer une **analyse en temps rÃ©el** avec Spark Streaming pour suivre les sentiments en direct.
* DÃ©ployer les composants sur le cloud (AWS, GCP ou Azure) pour passer Ã  lâ€™Ã©chelle.
* CrÃ©er un **dashboard interactif** (Power BI, Streamlit ou Superset) pour lâ€™exploration dynamique des donnÃ©es.
* Enrichir le modÃ¨le dâ€™analyse de sentiment avec des techniques de **Deep Learning** (BERT, LSTM, etc.).